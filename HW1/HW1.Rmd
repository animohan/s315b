---
title: "HW1"
author: "Anish Mohan"
date: "April 23, 2016"
output: html_document
---
1. Q1

```{r}
    Income=read.csv("Income_Data.txt")
    ModIncome=data.frame(Inc=Income$X9,sex=Income$X2,marital=Income$X1,age=Income$X5,edu=Income$X4,occ=Income$X5.1,dwelltime=Income$X5.2,dual=Income$X3,hh=Income$X3.1,hh18=Income$X0,house=Income$X1.1,hometype=Income$X1.2,Ethnic=Income$X7,lang=Income$NA.)
    
    Inc=factor(ModIncome$Inc, levels=1:9, labels=c("<10K","10-15K","15-20K","20-25K","25-30K","30-40K","40-50K","50K-75K",">75K"))
    sex=factor(ModIncome$sex, levels=1:2, labels=c("Male","Female"))
    marital=factor(ModIncome$marital, levels=1:5,labels=c("Married","live-in","Divorced","Seperated","Single"))
    age=factor(ModIncome$age,levels=1:7,labels=c("14-17","18-24","25-34","35-44","45-54","55-64","over 65"))
    edu=factor(ModIncome$edu,levels=1:6,labels=c("less grade 8","grade 9-11","grad high","1-3 college","College grad","Grad"))
    occ=factor(ModIncome$occ,levels=1:9,labels=c("Professional","Sales","laborer","Clerk","Home","Student","Military","Retired","Unemployed"))
    dwelltime=factor(ModIncome$dwelltime,levels=1:5,labels=c("<1year","1-3 years","4-6 years","7-10 years",">10 years"))
    dual=factor(ModIncome$dual, levels=1:3, labels=c("Not Married","Yes","No"))
    hh=factor(ModIncome$hh, levels=1:9, labels=c("1","2","3","4","5","6","7","8",">9"))
    hh18=factor(ModIncome$hh18, levels=1:9, labels=c("1","2","3","4","5","6","7","8",">9"))
    house=factor(ModIncome$house, levels=1:3, labels=c("Own","Rent","Live with family"))
    hometype=factor(ModIncome$house,levels=1:5, labels =c("House","Condo","Apa","Mobile","Other"))
    ethnic=factor(ModIncome$Ethnic, levels=1:8, labels=c("American Ind","Asian","Black","East indian","Hispanic","Pacific Island","White","Other"))
    lang=factor(ModIncome$lang,levels=1:3, labels=c("English","Spanish","Other"))
    
    FinalInc=data.frame(Inc=Inc,sex=sex,marital=marital,age=age,edu=edu,occ=occ, dwelltime=dwelltime, dual=dual, hh=hh, hh18=hh18,house=house, hometype=hometype,ethnic=ethnic, lang=lang)
    
    incfit=rpart(Inc~.-Inc,FinalInc)
    plot(incfit)
    text(incfit)
    summary(incfit)
```

  + 1.1 Short Summary on the results:
    + Occupation seems to be one of the key factors that influence the Annual Income. If the occupation is "Unemployed" or "Student" the predicted annual income is less than 10K. Next good predictor is if the family rents v.s owns the house. If the family owns the house, it more likely that their annual income is >=$50K. If the household rent or lives with a family, then age is a next important predictor of the annual income. For people in the age group 14-24 and people above 55 the predicted household income is below 10K
    
  + 1.a Yes, surrogate splits were used in the construction of optimal tree.
    
      + A surrogate split is used when a data point is missing the variable value which is used for decision about which branch the data point should be sent to. A surrogate is the value of another dimension/variable for the same data point; The surrogate value is used for making the splitting decision instead of the missing variable value.
      
      + In my decision tree, Occupation is the first variable used for splitting. However for 136 data points, occupation is not listed. In this case, age is used as a surrogate for occupation. Age and occupation give the split for ~86% of given datapoints

 + 1b.
    ```{r}
    mydata=data.frame(Inc=" ",sex="Male",marital="Married",age="35-44",edu="Grad",occ="Professional",dwelltime="1-3 years",dual="No",hh="3",hh18="3",house="Own",hometype="House",ethnic="Asian",lang="Other")
    tree.pred=predict(incfit,mydata, type="class")
    tree.pred
    ```


2. Q2
  
  
3. Q3

    + A Target function gives the true mapping from the Input variable space to the output variable space $f:X->Y$ If the true Target function is known, we can predict the value of Output/Response given an input variable.
    
    + Theoretically, A Target function should give an accurate function for prediction; However, even when a true target function is known, we might not be able to accurately predict values because of Noise.
    
    + Our goal is to get the best estimate of the target function $hat{Y}~Y=F(X)$. And for estimating this, we have data point pairs $<x_i,y_i$

4. Q4
  + Empirical risk evaluated on training data is a reasonable surrogate for the actual(population) prediction risk if certain assumptions are true:
    + Training data comes from the same distribution as the population data.
    
    + A learning method is not overfit to reduce the empirical risk only for the training data.
    
    + Generally, it is a good idea to keep a part of training set (test set) separate only to evaluate the empirical risk of a learning method. This test set is not used to tune the parameters of learning method but to act as a surrogate to estimate the empirical risk for the actual population.
    
5. Q5
  + Bias is the error introduced by approximating a complex target function with a simplet model which does not capture the true underlying form of the target function. Example approximating a quadratic function with least squares. Because we chose a simpler model, we will never be able to accurately estimate the target function beyond an error level. This is called the Bias of the model.
  
  + Variance is the amount by which a predicted function $hat{f}$ would change if we were using a different training data set. Different training sets give different value of the function and the variations in the function value is called the Variance.
  
  + Bias-Variance tradeoff: For finding the best function that approximates the target function we use different models or classes of function. Bias is introduced by choosing a particular class of functions to approximate the target function. Typically, more flexible a function class is, lower will be the bias associated with that function class. However, by increasing flexibility, variance of the output $hat{f}$ is high when we use different training set with this flexible function class. Bias-Variance tradeoff refers to this trade-off where following things happen:
    + compromise on increasing in Variance by choosing a more flexible class of functions to represent the target function
    + compromise on increase in Bias by choosing a simpler model which reduces the variance.
  
  + We manage the Bias-Variance trade-off to get a lower expected test error with the training data or test data.


 
 